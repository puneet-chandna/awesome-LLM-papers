# Chain-of-Thought Prompting - Detailed Summary

ğŸ“„ **Paper:** [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)  
ğŸ‘¥ **Authors:** Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou  
ğŸ›ï¸ **Institution:** Google Research, Brain Team  
ğŸ“… **Published:** January 2022 (NeurIPS 2022)

---

## ğŸ¯ One-Line Summary

Simply asking large language models to "think step by step" dramatically improves performance on complex reasoning tasks, unlocking emergent capabilities without any model changes.

## ğŸ” Problem Statement

Standard prompting with LLMs struggled with:

- **Complex reasoning:** Multi-step math, logic, common sense
- **Opaque failures:** Models gave wrong answers without explanation
- **Limited capabilities:** Even large models failed on simple arithmetic
- **No intermediate steps:** Direct input â†’ output, no reasoning visible

## ğŸ’¡ Key Innovation: Chain-of-Thought (CoT)

### The Breakthrough:

Instead of:

```
Q: Roger has 5 tennis balls. He buys 2 more cans of 3 balls each. How many balls does he have now?
A: 11
```

Use Chain-of-Thought:

```
Q: Roger has 5 tennis balls. He buys 2 more cans of 3 balls each. How many balls does he have now?
A: Roger started with 5 balls. 2 cans of 3 balls each is 6 balls. 5 + 6 = 11.
```

### Two Variants:

### 1. **Few-Shot CoT**

Provide 3-8 examples with step-by-step reasoning:

- Show the thinking process explicitly
- Model learns to mimic the reasoning pattern
- Works across different task types

### 2. **Zero-Shot CoT** (The Magic Phrase)

Just add: **"Let's think step by step."**

```
Q: If a store has 23 apples and sells 17, then gets 8 more, how many apples?
A: Let's think step by step.
```

Model automatically generates intermediate reasoning steps!

## ğŸ“Š Results & Impact

### Benchmark Performance:

| Task               | Standard Prompting | CoT Prompting | Improvement |
| ------------------ | ------------------ | ------------- | ----------- |
| **GSM8K (Math)**   | 17.9%              | **58.1%**     | +224%       |
| **SVAMP (Math)**   | 69.9%              | **79.0%**     | +13%        |
| **AQuA (Algebra)** | 33.1%              | **43.7%**     | +32%        |
| **CommonsenseQA**  | 67.3%              | **74.5%**     | +11%        |
| **StrategyQA**     | 60.9%              | **69.1%**     | +13%        |

### Why This Changed Everything:

1. **Zero-cost improvement:** No model retraining needed
2. **Emergent ability:** Only works with 100B+ parameter models
3. **Interpretable:** Can see model's reasoning process
4. **Universal technique:** Works across many task types
5. **Foundation for reasoning:** Led to entire research area

## ğŸ”® What Came After

This paper spawned:

- **Self-Consistency CoT** (2023): Sample multiple reasoning paths
- **Tree of Thoughts** (2023): Explore branching reasoning
- **Graph of Thoughts** (2023): Non-linear reasoning structures
- **ReAct** (2023): Reasoning + Acting for agents
- **OpenAI o1** (2024): Native CoT reasoning models
- **DeepSeek-R1** (2025): Open-source reasoning models

## ğŸ’» Implementation

```python
# Few-shot Chain-of-Thought prompting
few_shot_prompt = """
Q: Sam has 3 apples. He gives 1 to his friend. How many does he have?
A: Sam started with 3 apples. He gave away 1. 3 - 1 = 2. He has 2 apples.

Q: A train travels 60 mph for 2 hours. How far does it go?
A: Distance = speed Ã— time. 60 mph Ã— 2 hours = 120 miles.

Q: {your_question}
A:
"""

# Zero-shot Chain-of-Thought (the magic phrase!)
zero_shot_prompt = f"""
Q: {your_question}
A: Let's think step by step.
"""

# The model will automatically generate reasoning steps!
```

## ğŸ¯ When CoT Works Best

**âœ… Great for:**

- Multi-step arithmetic and math
- Logical reasoning problems
- Commonsense reasoning chains
- Complex question answering
- Planning and strategy

**âŒ Less useful for:**

- Simple factual lookup ("What's the capital of France?")
- Single-step tasks
- Creative writing
- Small models (<10B parameters)

## ğŸ’¡ CoT Variants & Extensions

### **Self-Consistency CoT:**

Generate multiple reasoning paths and take majority vote

### **Least-to-Most Prompting:**

Break complex problems into simpler sub-problems

### **Auto-CoT:**

Automatically generate CoT examples using clustering

### **Program-aided CoT:**

Generate Python code for computational steps

## ğŸ“ Key Takeaways

- **"Let's think step by step"** is surprisingly powerful
- Emergent ability: Only appears at scale (100B+ params)
- Reasoning can be elicited through prompting alone
- Transparency: Seeing reasoning helps debug failures
- Free lunch: No training, just better prompts
- Foundation for modern reasoning systems (o1, DeepSeek-R1)

## ğŸ“š Essential Resources

- [Original Paper](https://arxiv.org/abs/2201.11903)
- [Google AI Blog](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html) - Official announcement
- [The Illustrated Chain-of-Thought](https://jalammar.github.io/tag/chain-of-thought/) - Visual explanation
- [Prompting Guide](https://www.promptingguide.ai/techniques/cot) - Practical tutorial
- [Learn Prompting Course](https://learnprompting.org/docs/intermediate/chain_of_thought) - Interactive lessons
- [Self-Consistency Paper](https://arxiv.org/abs/2203.11171) - Follow-up work
- [Tree of Thoughts](https://arxiv.org/abs/2305.10601) - Extension to branching reasoning

## ğŸ“ This summary is part of [Awesome LLM Papers](https://github.com/puneet-chandna/awesome-LLM-papers) - Star us for Weekly research updates!
