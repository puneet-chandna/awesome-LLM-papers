# ğŸ§  Awesome LLM Papers

<div align="center">

![Daily LLM Papers](https://img.shields.io/badge/Daily%20LLM%20Papers-Breaking%20Research%20Daily-blue?style=for-the-badge&logo=arxiv&logoColor=white)
[![Stars](https://img.shields.io/github/stars/puneet-chandna/awesome-LLM-papers?style=for-the-badge&color=yellow)](https://github.com/puneet-chandna/awesome-LLM-papers/stargazers)
[![Updates](https://img.shields.io/badge/Updates-Daily%20@%209AM%20IST-green?style=for-the-badge)](https://github.com/puneet-chandna/awesome-LLM-papers)
[![Newsletter](https://img.shields.io/badge/Newsletter-500+%20Subscribers-red?style=for-the-badge)](https://your-newsletter-link.com)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=for-the-badge)](CONTRIBUTING.md)

<h3>Read what matters. Skip the noise. ğŸ¯</h3>

<p align="center">
  <a href="#-todays-pick">Today's Pick</a> â€¢
  <a href="#-this-weeks-essential-reads">This Week</a> â€¢
  <a href="#-must-read-papers-hall-of-fame">Hall of Fame</a> â€¢
  <a href="#-browse-by-category">Categories</a> â€¢
  <a href="#-contributing">Contribute</a>
</p>

---

<img src="https://img.shields.io/badge/ğŸ“„%20Papers%20Curated-127-blue?style=flat-square&labelColor=000000" alt="Papers">
<img src="https://img.shields.io/badge/ğŸ†•%20Added%20This%20Week-8-green?style=flat-square&labelColor=000000" alt="New Papers">
<img src="https://img.shields.io/badge/ğŸ‘¥%20Contributors-23-orange?style=flat-square&labelColor=000000" alt="Contributors">
<img src="https://img.shields.io/badge/â­%20Community%20Rated-500+-red?style=flat-square&labelColor=000000" alt="Ratings">

</div>

---

## ğŸ”¥ Today's Pick

> **January 15, 2025** | Fresh breakthrough @ 9:00 AM PST

### ğŸ† **[Attention Bottlenecks: Rethinking Transformer Efficiency at Scale](https://arxiv.org/abs/2025.xxxxx)**

<table>
<tr>
<td width="70%">

**Authors:** *Sarah Chen, Marcus Wu et al.* â€¢ MIT & Google DeepMind

**Why this matters:** This paper introduces "Bottleneck Attention," reducing memory consumption by 70% while maintaining 99% of performance. This makes 100B+ parameter models trainable on consumer GPUs for the first time.

**Key Innovations:**
- ğŸ”¸ Novel attention mechanism that processes only critical tokens
- ğŸ”¸ Dynamic token pruning based on information density
- ğŸ”¸ Backwards compatible with existing transformer architectures

</td>
<td width="30%">

**Resources:**
- ğŸ”— [**Code**](https://github.com/example/bottleneck-attention)
- ğŸ“Š [**Demo**](https://huggingface.co/spaces/example)
- ğŸ¦ [**Thread**](https://twitter.com/author/status/xxx)
- ğŸ“ [**Our Summary**](summaries/bottleneck-attention.md)

**Impact Score:** 
```diff
+ Performance: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 85%
+ Innovation:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 92%
+ Practicality:â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 98%
```

</td>
</tr>
</table>

---

## ğŸ“† This Week's Essential Reads

<details open>
<summary><b>Click to expand this week's papers</b> (January 13-19, 2025)</summary>

<br>

| Day | Paper | Impact | TL;DR |
|-----|-------|--------|--------|
| **Mon** | [ğŸ§® Chain-of-Verification: Self-Correcting LLM Reasoning](https://arxiv.org) | `reasoning` `accuracy` | Reduces hallucination by 65% through iterative self-verification |
| **Tue** | [âš¡ FlashAttention-3: 10x Faster Training](https://arxiv.org) | `efficiency` `training` | New hardware-aware algorithm makes training 10x faster |
| **Wed** | [ğŸ¨ DALL-E 3: Consistent Character Generation](https://arxiv.org) | `multimodal` `vision` | Maintains character consistency across multiple generations |
| **Thu** | [ğŸ›¡ï¸ Constitutional RL: Safer RLHF Training](https://arxiv.org) | `safety` `alignment` | Reduces harmful outputs by 89% during training |
| **Fri** | [ğŸ—ï¸ Mixture of Depths: Dynamic Computation](https://arxiv.org) | `architecture` `efficiency` | Adaptively uses layers based on input complexity |

</details>

---

## ğŸ“š Must-Read Papers (Hall of Fame)

> ğŸ›ï¸ **Papers that fundamentally changed the field**

<table>
<thead>
<tr>
<th width="30%">Paper</th>
<th width="15%">Impact</th>
<th width="40%">Why Essential</th>
<th width="15%">Resources</th>
</tr>
</thead>
<tbody>

<tr>
<td>

**[Attention Is All You Need](https://arxiv.org/abs/1706.03762)**
<br>*Vaswani et al., 2017*

</td>
<td align="center">

ğŸ† **Foundational**
<br>`architecture`

</td>
<td>

Created the Transformer architecture that powers all modern LLMs. Replaced RNNs with self-attention, enabling parallelization and scaling.

</td>
<td align="center">

[ğŸ“„](https://arxiv.org/abs/1706.03762) [ğŸ’»](https://github.com/tensorflow/tensor2tensor) [ğŸ“Š](http://jalammar.github.io/illustrated-transformer/)

</td>
</tr>

<tr>
<td>

**[GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)**
<br>*Brown et al., 2020*

</td>
<td align="center">

ğŸš€ **Scale**
<br>`emergence`

</td>
<td>

Proved that scale leads to emergent abilities. In-context learning without fine-tuning revolutionized how we use LLMs.

</td>
<td align="center">

[ğŸ“„](https://arxiv.org/abs/2005.14165) [ğŸ”](https://openai.com/api/) [ğŸ“Š](https://gpt3demo.com)

</td>
</tr>

<tr>
<td>

**[Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)**
<br>*Bai et al., 2022*

</td>
<td align="center">

ğŸ›¡ï¸ **Safety**
<br>`alignment`

</td>
<td>

Introduced RLAIF - training AI systems to be helpful and harmless using AI feedback instead of human feedback.

</td>
<td align="center">

[ğŸ“„](https://arxiv.org/abs/2212.08073) [ğŸ’»](https://github.com/anthropics/constitutional-ai) [ğŸ¥](https://youtube.com/watch)

</td>
</tr>

<tr>
<td>

**[Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)**
<br>*Wei et al., 2022*

</td>
<td align="center">

ğŸ§® **Reasoning**
<br>`prompting`

</td>
<td>

Simple prompting technique that dramatically improves reasoning by asking models to think step-by-step.

</td>
<td align="center">

[ğŸ“„](https://arxiv.org/abs/2201.11903) [ğŸ’¡](https://github.com/examples) [ğŸ“Š](https://demo.com)

</td>
</tr>

<tr>
<td>

**[RLHF: Training with Human Feedback](https://arxiv.org/abs/1706.03741)**
<br>*Christiano et al., 2017*

</td>
<td align="center">

ğŸ¯ **Alignment**
<br>`training`

</td>
<td>

The technique behind ChatGPT's success. Aligns model outputs with human preferences through reinforcement learning.

</td>
<td align="center">

[ğŸ“„](https://arxiv.org/abs/1706.03741) [ğŸ’»](https://github.com/openai/lm-human-preferences) [ğŸ“š](https://huggingface.co/blog/rlhf)

</td>
</tr>

</tbody>
</table>

---

## ğŸ·ï¸ Browse by Category

<div align="center">

| [ğŸ—ï¸ **Architecture & Models**](#architecture--models) | [ğŸ§® **Reasoning & Logic**](#reasoning--logic) | [âš¡ **Efficiency & Speed**](#efficiency--speed) |
|:---:|:---:|:---:|
| Transformers, SSMs, Novel Architectures | CoT, Planning, Mathematical Reasoning | Quantization, Pruning, Distillation |
| **28 papers** | **19 papers** | **22 papers** |

| [ğŸ›¡ï¸ **Safety & Alignment**](#safety--alignment) | [ğŸ¨ **Multimodal**](#multimodal) | [ğŸ”§ **Fine-tuning & PEFT**](#fine-tuning--peft) |
|:---:|:---:|:---:|
| RLHF, Constitutional AI, Red-teaming | Vision-Language, Audio, Video | LoRA, Adapters, Instruction Tuning |
| **31 papers** | **15 papers** | **12 papers** |

</div>

### Architecture & Models

<details>
<summary><b>View papers</b> (28 total)</summary>

- ğŸ†• [Mamba: Linear-Time Sequence Modeling](https://arxiv.org) - State-space models challenging transformers
- [Retentive Networks: Successor to Transformers](https://arxiv.org) - O(1) inference complexity
- [Mixture of Experts Revisited](https://arxiv.org) - Efficient scaling with specialized sub-networks
- [View all â†’](categories/architecture.md)

</details>

### Reasoning & Logic

<details>
<summary><b>View papers</b> (19 total)</summary>

- ğŸ”¥ [Tree of Thoughts: Deliberate Problem Solving](https://arxiv.org) - Explores multiple reasoning paths
- [Self-Consistency Improves CoT](https://arxiv.org) - Sample multiple paths, take majority vote
- [Program-aided Language Models](https://arxiv.org) - Use code interpretation for math
- [View all â†’](categories/reasoning.md)

</details>

### Efficiency & Speed

<details>
<summary><b>View papers</b> (22 total)</summary>

- â­ [QLoRA: 4-bit Quantization for Fine-tuning](https://arxiv.org) - Democratizes LLM fine-tuning
- [FlashAttention-2: Faster, Better](https://arxiv.org) - 2x speedup over original
- [Speculative Decoding](https://arxiv.org) - 2-3x faster inference without quality loss
- [View all â†’](categories/efficiency.md)

</details>

---

## ğŸ“Š Research Trends

<div align="center">

```mermaid
%%{init: {'theme':'dark'}}%%
graph LR
    A[2023] -->|Efficiency| B[2024]
    B -->|Multimodal| C[2025]
    
    style A fill:#1f2937
    style B fill:#374151
    style C fill:#4b5563
```

**Current Hot Topics:** ğŸ”¥ Long Context (>1M tokens) | ğŸ”¥ Reasoning without CoT | ğŸ”¥ Efficient Fine-tuning

</div>

---

## ğŸ¤ Contributing

<div align="center">

### **Add a paper in 30 seconds!**

<a href="https://github.com/puneet-chandna/awesome-LLM-papers/issues/new?assignees=&labels=new-paper&template=new-paper.yml&title=%5BPaper%5D%3A+">
  <img src="https://img.shields.io/badge/Submit%20Paper-Click%20Here-blue?style=for-the-badge&logo=github" alt="Submit Paper">
</a>

**Just need:** Paper link + One sentence on why it matters

âœ… Reviewed within 24 hours | ğŸ† Contributors get credit | ğŸ’¬ Join discussions

</div>

<!--


## ğŸ“§ Stay Updated

<div align="center">

| **Daily Updates** | **Weekly Digest** | **Community** |
|:---:|:---:|:---:|
| â­ Star & Watch this repo | ğŸ“§ [Newsletter](https://newsletter-link.com) (500+ subscribers) | ğŸ’¬ [Discord](https://discord.gg/xxxxx) (200+ members) |
| Get notifications for daily picks | Curated weekend reading | Discuss papers with researchers |

</div>

---

## ğŸ™ Acknowledgments

<div align="center">

### Top Contributors This Month

<a href="https://github.com/contributor1"><img src="https://github.com/contributor1.png" width="50" height="50" style="border-radius: 50%;" alt="contributor1"/></a>
<a href="https://github.com/contributor2"><img src="https://github.com/contributor2.png" width="50" height="50" style="border-radius: 50%;" alt="contributor2"/></a>
<a href="https://github.com/contributor3"><img src="https://github.com/contributor3.png" width="50" height="50" style="border-radius: 50%;" alt="contributor3"/></a>
<a href="https://github.com/contributor4"><img src="https://github.com/contributor4.png" width="50" height="50" style="border-radius: 50%;" alt="contributor4"/></a>
<a href="https://github.com/contributor5"><img src="https://github.com/contributor5.png" width="50" height="50" style="border-radius: 50%;" alt="contributor5"/></a>

 **Special thanks to our [23 contributors](https://github.com/yourusername/daily-papers-llm/graphs/contributors)** 

</div>
 -->
---

<div align="center">

**[â¬† Back to Top](#-awesome-LLM-papers)**

Made with â¤ï¸ for the AI Research Community

*Last updated: August 10, 2025, 9:00 AM IST*

[![Follow on Twitter](https://img.shields.io/twitter/follow/puneet_chandna_?style=social)](https://x.com/puneet_chandna_)
[![GitHub followers](https://img.shields.io/github/followers/puneet-chandna?style=social)](https://github.com/puneet-chandna)

</div>
